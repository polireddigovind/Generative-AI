{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files to process\n",
      "\n",
      "Processing: RETAIL_project.pdf\n",
      "  ✓ Loaded 18 pages\n",
      "\n",
      "Processing: aws.pdf\n",
      "  ✓ Loaded 3 pages\n",
      "\n",
      "Processing: Govind_AS1552_Snowflake_Asgn.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Processing: Airflow_tejas_final.pdf\n",
      "  ✓ Loaded 7 pages\n",
      "\n",
      "Processing: Govind RETAIL Documentation.pdf\n",
      "  ✓ Loaded 24 pages\n",
      "\n",
      "Total documents loaded: 67\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdffiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 67 documents into 73 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: RETAIL – CASE - STUDY \n",
      "NAME: Govind Polireddi      Employee Id: AS1552 \n",
      " \n",
      "Problem Statement \n",
      "Retail companies struggle with integrating in-store POS transactions, online orders, and \n",
      "inventory systems...\n",
      "Metadata: {'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='RETAIL – CASE - STUDY \\nNAME: Govind Polireddi      Employee Id: AS1552 \\n \\nProblem Statement \\nRetail companies struggle with integrating in-store POS transactions, online orders, and \\ninventory systems into a single analytics-ready platform. \\nThey need: \\n• Batch processing for daily sales reconciliation. \\n• Real-time ingestion for fraud detection & stock alerts. \\n• Cloud-native scalability to support peak season sales. \\n• Unified warehouse (Snowflake / Synapse) for advanced reporting  \\n• Monitoring & error handling for operational resilience. \\n• Data security for customer PII. \\n• Cost transparency for infrastructure & query optimization. \\n \\nAPPROACH: \\n➢ Step-1: Creating a ADLS storage account to store the raw data and also the cleaned \\n and validated data in medallion structure ( Bronze, Silver, Gold ). \\n➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for \\n              each of the Batch Orders CSV and Payments CSV files.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for \\n              each of the Batch Orders CSV and Payments CSV files.  \\n➢ Step-3: Use Event- Grid to create trigger the pipelines when ever there is new file     \\n               uploaded. \\n➢ Step-4: Copy the data to the bronze by using COPY activity, after clean and validate  \\nthe data and store it in the Silver and Gold layer and finally load it into the  \\nSnowflake table. \\n➢ Step-4: Create Alerts from ADF monitor and LOG the DATABRICKS logs into LOG  \\n              ANALYTICS WORKSPACE. \\n➢ Step-5: For STREAM-PROCESSING create a Event-hub to collect the events from the \\n              Kafka. Create Databricks Notebook and connect to the Event-hub. Collect the \\n events from event hub and  process the event and store the data in delta  \\nformat in bronze, silver and gold. Finally connect the snowflake toload the  \\ndata into the fact table.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='events from event hub and  process the event and store the data in delta  \\nformat in bronze, silver and gold. Finally connect the snowflake toload the  \\ndata into the fact table. \\n➢ Step-6: Load the customer, product and store data from ADLS to the snowflake by'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='ADLS SAS token and creating stage in the snowflake to copy the data. Use \\ncopy into the Snowflake table. \\n➢ Step-7: For the customer data do the PII masking for the Email and phone number   \\nfor security purpose. \\n➢ Step-8: Connect snowflake with Power BI for the real time analytics. \\n \\nARCHITECTURE: \\nI am using the below architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='AZURE STORAGE ACCOUNT CREATION: \\n✓ Created an storage account “azurestorageaccount52”.  \\n \\n✓ Create a container and folders in it to store the data in medallion Architecture. \\n \\n✓ Store the customer, products and store data in raw directory \\n \\n✓ The raworders and rawpayment folders are for the storage of Batch process files \\nautomatically when there is a new file uploaded.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='BATCH – PROCESSING: \\nFor Batch orders pipeline. \\n✓ Create a pipeline for batch orders to run when a new file is added in raworders. \\n \\nFor Batch payment pipeline \\n✓ Create a pipeline for batch payment to run when a new file is added in rawpayments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Create 2 triggers to run the pipelines for new file upload. \\n \\n \\n✓ Create Alerts for the pipeline failures and activity failures for both the pipelines.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='CREATE ACTION GROUP: \\n✓ Created action group for notification alerts through email. \\n \\n \\n \\nOrders Batch processing Notebook:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='OUTPUT TABLE ( FACT ORDER ):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='Payments Batch processing Notebook: \\n \\n \\n \\n \\nOUTPUT TABLE ( PAYMENTS ):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='STREAM– PROCESSING: \\n✓ Create an Event hub to collect events and send to the databricks notebook \\n \\n✓ Create a new event in the event hub namespace “events” \\n \\n✓ Create a notebook and make connection with storage account.  \\n✓ USE THE AZURE KEY – VAULT to STORE THE SECRETS.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Connect to the event hub  \\n \\n✓ Schema define for the data \\n \\n \\n✓ Loading data to the bronze layer which are received from the event hub events. \\n✓ Separate the events as per the orders and Inventory data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Broze to silver after cleaning and validating the data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Gold to snowflake table'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ INVENTORY UPDATE TO THE SNOWFLAKE INVENTORY TABLE \\n \\n \\nSNOWFLAKE WAREHOUSE DATA TABLE: \\n✓ E - R DIAGRAM:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Snowflake tables'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Loading data from ADLS to a stage in Snowflake and copy into the tables \\n \\nPII MASKING: \\n✓ For the customer table mask the email and phone number'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='Customer Table: \\n \\n \\nMONITORING : \\n✓ Create a log analytics workspace \\n \\n✓ Open LOGS and then use KQL mode'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Query the logs  \\n \\n \\nALERTS: \\n✓ Create alerts for pipeline failures and activity failures'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 17, 'page_label': '18', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='POWER BI: \\n✓ Received more payments through CARD'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='AWS ASSESSMENT\\nPOLIREDDI GOVIND                                                                                 AS1552\\nProblem Statement: \\nBuild a Batch Data Pipeline to Ingest CSV Data from S3 → Process with Glue → Load to Redshift → Query via Athena\\nSTEP 1:  SERVICE CREATION + IAM ROLES\\n➢ Create  three  S3 buckets (Source CSEV storage, Failed CSV storage, Processed CSV storage) to store the CSV ﬁles.\\n➢ Create a GlueRole in IAM Roles with AWSGlueServiceRole and AmazonS3FullAccess policies.\\n➢ Create a RedshiftRole in IAM Roles with AmazonS3ReadOnlyAccess and AWSGlueConsoleFullAccess policies.\\n➢ Create a new Redshift serverless namespace and workgroup, attach the RedshiftRole to grant the necessary permissions \\nto access the S3 and AWS Glue Data Catalog.\\n➢ Create a Glue Crawler and add the S3 bucket as data source, attach GlueRole policy to grant the required permissions to \\naccess the data.\\nSTEP 2:  AUTOMATION WITH LAMBDA + STEP FUNCTIONS'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='➢ Create a Glue Crawler and add the S3 bucket as data source, attach GlueRole policy to grant the required permissions to \\naccess the data.\\nSTEP 2:  AUTOMATION WITH LAMBDA + STEP FUNCTIONS\\n➢ When ever a new CSV ﬁle is added to the S3 bucket a LAMBDA function will get triggered.\\nCSV S3  \\nBUCKET\\nGLUE  \\nETL \\nJOB\\nRED SHIFT ATHENA'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='➢ The LAMBDA function starts the AWS Step Functions  workﬂow\\n➢ Step Functions orchestrate the pipeline:\\n• Start Glue Job and Monitor job completion.\\n• Trigger next steps automatically.\\n➢ Failed ﬁles will be directed to Failed_CSV_S3 storage and then by using AWS SNS a notiﬁcation will be sent.\\nSTEP 3: CRAWLER + DATA PROCESSING\\n➢ The raw CSV data stored in the Amazon S3 bucket will be crawled then metadata and schema will be stored in GLUE \\nCATALOG and then ingested to an AWS Glue ETL job.\\n➢ The Glue job will extract, clean, and transform the data, then convert it into an Parquet format. \\n➢ The processed Parquet ﬁles will be stored in a separate Amazon S3 bucket.\\nSTEP 4: LOADING PARQUT FILES INTO AMAZON REDSHIFT\\n➢ The processed Parquet ﬁles from the new Amazon S3 bucket are loaded into Amazon Redshift with(COPY/Auto COPY \\nfrom processed Parquet).\\n➢ Use IAM role to allow Redshift to read processed bucket.\\nSTEP 5: CONNECTING TO AMAZON ATHENA'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='from processed Parquet).\\n➢ Use IAM role to allow Redshift to read processed bucket.\\nSTEP 5: CONNECTING TO AMAZON ATHENA\\n➢ Deﬁne Athena Workgroup and connect it with Redshift Spectrum for querying processed data.\\nCloudFormation for the Pipeline: \\nThis CloudFormation stack sets up an end-to-end data processing pipeline on AWS. It provisions the required S3 \\nbuckets, IAM roles, Glue crawlers, Glue ETL jobs, and Redshift Serverless resources. The pipeline automates \\ningestion of CSV ﬁles using Lambda and Step Functions, transforms them into Parquet format, and loads the \\nprocessed data into Amazon Redshift and integrates with Amazon Athena for querying and analytics.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='ARCHITECTURE'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='SNOWFLAKE ASSIGNMENT \\n \\nName: Polireddi Govind Employee Id: AS1552 \\n \\nProblem Statement 1: \\nAutomate Real-time Log Data Ingestion from AWS S3 into \\nSnowflake using Snowpipe and Monitor via Snowsight  \\n \\nTopics Covered: Snowpipe, Internal/External Stage, Copy Options, \\nStreaming, SnowSQL, Performance Optimization, Virtual Warehouse, \\nCaching, Monitoring (Snowsight), AWS S3 Integration, Time Travel,  \\nClustering, Data Sampling \\n \\nProject Overview: \\nThis project aims to automate real-time log data ingestion from AWS S3 into Snowflake using Snowpipe, \\nestablishing a seamless, serverless, and near-instant data loading pipeline. By connecting Snowflake with S3 \\nevent notifications, newly uploaded log files are automatically ingested, processed, and made available for \\nanalytics with minimal manual effort. The solution focuses on performance optimization, cost efficiency, and \\noperational visibility, leveraging Snowflake’s capabilities such as external/internal stages, copy options, virtual'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='operational visibility, leveraging Snowflake’s capabilities such as external/internal stages, copy options, virtual \\nwarehouses, clustering, caching, time travel, and Snowsight monitoring.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='Architecture Overview \\n1. Log Producers → AWS S3 \\n• Application logs (e.g., auth, payments, orders, shipping) are written to S3 in JSON format, one log per \\nline. \\n• Each JSON contains fields like: \\no log_id, timestamp, level, service, message, user_id, ip_address.  \\n• File size target: 10–200 MB compressed (parquet) for efficient Snowpipe ingestion. \\n \\n2. S3 Event Notifications → Snowflake Snowpipe \\n• Each new file triggers an S3 event notification (via SNS/SQS). \\n• Snowpipe automatically loads data into Snowflake without manual intervention. \\n3. Snowflake Staging and Curation \\n• External Stage: Secure pointer to the S3 bucket/prefix. \\n• Snowpipe: Automatically ingests JSON logs into a raw staging table. \\n• Curation Layer: Transforms, normalizes, and deduplicates data into curated analytics tables. \\n4. Monitoring and Optimization \\n• Snowsight: Monitors ingestion latency, file load history, and error counts. \\n• Virtual Warehouses: Handle downstream transformations.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='4. Monitoring and Optimization \\n• Snowsight: Monitors ingestion latency, file load history, and error counts. \\n• Virtual Warehouses: Handle downstream transformations. \\n• Clustering & Time Travel: Improve query speed and allow recovery from data issues.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content=\"Step-by-Step Approach for Implementation  \\nStep 1: Data Landing on S3 \\n• Logs generated by different services are streamed to the S3 bucket.  \\n• Files are batched and upload to the S3 bucket. \\n                              \\nStep 2: Setting Up the Snowflake Stage \\n• Create an AWS IAM role with GetObject and ListBucket permissions. \\n• Stage points securely to the S3 log location. \\n                   \\nStep 3: Defining File Format and Copy Options \\n• Define a JSON file format to handle the log data structure: \\n• COPY options: \\no Use ON_ERROR='CONTINUE' to skip corrupted records. \\no Enable PURGE=FALSE for reprocessing flexibility. \\nStep 4: Automating Ingestion with Snowpipe\"),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='• Create a Snowpipe with AUTO_INGEST = TRUE: \\n• Configure S3 event notifications to publish to the Snowflake pipe endpoint. \\n• Each new JSON file is ingested into the raw_logs table. \\n                               \\nStep 5: Transforming and Curating Data \\n• Parse JSON data into structured columns: \\n• Apply deduplication using DISTINCT or a merge on log_id. \\n• Add clustering keys: (event_time, service_name).  \\n• Use Time Travel for data correction and auditing. \\n                                \\nStep 6: Performance Optimization \\n• Avoid small files — batch logs into optimal file sizes. \\n• Use clustering and search optimization for fast filtering (e.g., by service_name or log_level).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='• Enable caching for repeated dashboard queries.  \\n• Periodically re-cluster curated tables to maintain performance. \\n                                     \\nStep 7: Monitoring with Snowsight \\n• Use the Pipes Dashboard to track: \\no Ingestion latency \\no Success/failure rates \\no File queue backlog \\n• Set up custom KPIs: \\no Ingestion delay = CURRENT_TIMESTAMP - file_last_modified \\no Failed file counts, ingestion throughput \\n• Configure email/SMS alerts for load errors or pipe downtime.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='Step 8: Backfills and Reprocessing \\n• Use COPY INTO for historical or missed data loads: \\n• Pause/resume AUTO_INGEST during maintenance.  \\n• Use Time Travel and Fail-safe for replay or compliance scenarios. \\n \\nStep 9: Advanced Capabilities \\n• Snowpipe Streaming: For sub-second ingestion directly from producers (optional). \\n• Data Sampling: Use SAMPLE or partition filters for debugging.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='• Cost Control: \\no Use Snowpipe for ingestion (Snowflake -managed compute). \\no Use auto-suspend warehouses for transformation to minimize compute cost.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='ARCHITECTURE: \\nS3 → SQS → Snowpipe  → Snowflake Stage → Copy Into Logs Table (Snowflake)  \\n \\n  \\nIMPLEMENTATION \\n \\nS3 BUCKET: \\n✓ Created a S3 bucket “snowflake-logs-data-bucket” \\n \\nSNOWFAKE WAREHOUSE: \\n✓ Created a Warehouse (Sigmoid) , Database(logsdb) and Schema (RAW). \\n \\n✓ Created a log_events Table to store the logs from S3. \\n✓ Create a JSON file format.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='S3 READ POLICY(Role): \\n✓ Create a new role in IAM for S3 read access and assign to the Snowflake. \\n \\n \\n \\nS3 and SNOWFLAKE INTEGRATION: \\n✓ Intregrate S3 and Snowflake, assign the role and the location of the bucket. \\n \\n✓ Describe the created integration. \\n \\n✓ Copy the ARN and External_ID from the OUTPUT.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Update the Policy with ARN and External_ID in the Role policy. \\n \\nSTAGE CREATION: \\n✓ Create a stage to point it to the S3 bucket to fetch the Json files. \\n \\n✓ Successfully created a stage. \\n \\nSNOWPIPE: \\n✓ Create a Snowpipe to ingest the data continuously into the logs table.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Lineage graph is created from the Stage to the Table \\n \\n \\n \\nSQS EVENT NOTIFICATION: \\n✓ Create SQS Event notification ,when there is a new file added into the S3 bucket it \\ngets trigger the Snowpipe. \\n \\n✓ Successfully created the SQS notification.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='FILE UPLOAD: \\n✓ Uploaded a “logs_dataset1.json” file to the S3 bucket. \\n \\n✓ Successfully loaded the logs data into the Table (100 Rows) \\n \\n \\n✓ Uploaded a “logs_dataset2.json” file to the S3 bucket.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Successfully loaded the logs data into the Table (200 Rows) \\n \\n \\nTIME TRAVEL: \\n✓ Retention period is 7 days. \\n \\n✓ Table logs data is of 200 Rows.(Present) \\n \\n✓ Table logs data is of 100 Rows.(past of [400] seconds )'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='CACHE AND QUERY OPTIMIZATION: \\n✓ Cluster the Column(timestamp) for faster optimization. \\n \\n✓ 90ms time taken to retrieve the data as per the Query. \\n \\n✓ 38ms time taken to retrieve the data as per the Query as it cached the Result(Result \\nCache). \\n \\n \\n \\nMONITORING(SNOWSIGHT):  \\n✓ Number of logs per day. 28th September got more logs.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Error is having the main issue in the logs. \\n \\n✓ Bar chart showing the message wise total logs.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT    1.   Why  I  pasted  it:  To  show  the  raw  and  processed  storage  locations  I  set  up  for      \\nmy\\n \\npipeline.\\n \\n What  I  did  there:  Created  an  S3  bucket  named  my-store-sales-pipeline with  \\nfolders\\n RAW/ for  input  JSON  files,  Processed/ for  Parquet  outputs,  and  athena_results/ for  query  results.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT    \\n      2.  Why  I  pasted  it:  To  demonstrate  that  I  configured  Athena  to  use  the  Glue  Data  \\nCatalog\\n \\ndatabase\\n \\nfor\\n \\nquerying\\n \\nprocessed\\n \\ndata.\\n \\n What  I  did  there:  Opened  Athena  Query  Editor  and  confirmed  that  the  database  store_sales_db is  visible  and  available  for  running  queries.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT    \\n   3.  Why  I  pasted  it:  To  show  the  local  environment  setup  needed  to  run  Airflow  \\nwithout\\n \\nusing\\n \\npaid\\n \\nMWAA.\\n \\n What  I  did  there:  Installed  Docker  Desktop,  pulled  the  official  Airflow  Docker  \\nCompose\\n \\nsetup,\\n \\nand\\n \\nprepared\\n \\nthe\\n dags/,  plugins/,  and  requirements/ folders  for  \\nthe\\n \\nproject.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT      4.  Why  I  pasted  it:  To  demonstrate  that  my  DAG  executed  successfully,  \\norchestrating\\n \\nthe\\n \\nJSON-to-Parquet\\n \\nconversion\\n \\nand\\n \\nAthena\\n \\ntable\\n \\nrefresh.\\n \\n What  I  did  there:  Triggered  the  store_sales_reporting DAG  in  the  Airflow  UI,  \\nand\\n \\nboth\\n \\ntasks\\n \\n(convert_json_to_parquet and  run_athena_query)  completed  \\nsuccessfully.\\n  \\n   5.  Why  I  pasted  it:  To  validate  that  the  DAG  successfully  transformed  raw  JSON  \\nfiles\\n \\ninto\\n \\nParquet\\n \\nformat\\n \\nand\\n \\nstored\\n \\nthem\\n \\nback\\n \\nin\\n \\nS3.\\n \\n What  I  did  there:  Checked  the  Processed/ folder  inside  my  S3  bucket  and  \\nconfirmed\\n \\nthat\\n \\nnew\\n .parquet files  were  created  after  the  DAG  run.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='RETAIL – CASE - STUDY \\nNAME: Govind Polireddi      Employee Id: AS1552 \\n \\nProblem Statement \\nRetail companies struggle with integrating in-store POS transactions, online orders, and \\ninventory systems into a single analytics-ready platform. \\nThey need: \\n• Batch processing for daily sales reconciliation. \\n• Real-time ingestion for fraud detection & stock alerts. \\n• Cloud-native scalability to support peak season sales. \\n• Unified warehouse (Snowflake / Synapse) for advanced reporting  \\n• Monitoring & error handling for operational resilience. \\n• Data security for customer PII. \\n• Cost transparency for infrastructure & query optimization. \\n \\nI am Using LAMBDA ARCHITECTURE as it supports both BATCH AND STREAMING PROCESS. \\n \\nAPPROACH: \\n➢ Step-1: Creating a ADLS storage account to store the raw data and also the cleaned \\n and validated data in medallion structure ( Bronze, Silver, Gold ). \\n➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='and validated data in medallion structure ( Bronze, Silver, Gold ). \\n➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for \\n              each of the Batch Orders CSV and Payments CSV files.  \\n➢ Step-3: Use Event- Grid to create trigger the pipelines when ever there is new file     \\n               uploaded. \\n➢ Step-4: Copy the data to the bronze by using COPY activity, after clean and validate  \\nthe data and store it in the Silver and Gold layer and finally load it into the  \\nSnowflake table. \\n➢ Step-4: Create Alerts from ADF monitor and LOG the DATABRICKS logs into LOG  \\n              ANALYTICS WORKSPACE. \\n➢ Step-5: For STREAM-PROCESSING create a Event-hub to collect the events from the \\n              Kafka. Create Databricks Notebook and connect to the Event-hub. Collect the \\n events from event hub and  process the event and store the data in delta'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='format in bronze, silver and gold. Finally connect the snowflake toload the  \\ndata into the fact table. \\n➢ Step-6: Load the customer, product and store data from ADLS to the snowflake by \\nADLS SAS token and creating stage in the snowflake to copy the data. Use \\ncopy into the Snowflake table. \\n➢ Step-7: For the customer data do the PII masking for the Email and phone number   \\nfor security purpose. \\n➢ Step-8: Connect snowflake with Power BI for the real time analytics. \\n \\n \\nARCHITECTURE: \\nI am using the below architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='AZURE STORAGE ACCOUNT CREATION: \\n✓ Created an storage account “azurestorageaccount52”.  \\n \\n✓ Create a container and folders in it to store the data in medallion Architecture. \\n \\n✓ Store the customer, products and store data in raw directory \\n \\n✓ The raworders and rawpayment folders are for the storage of Batch process files \\nautomatically when there is a new file uploaded.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='BATCH – PROCESSING: \\nFor Batch orders pipeline. \\n \\n✓ Create a pipeline for batch orders to run when a new file is added in raworders. \\n \\n \\n \\nFor Batch payment pipeline'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Create a pipeline for batch payment to run when a new file is added in rawpayments. \\n \\n✓ Create 2 triggers to run the pipelines for new file upload.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Create Alerts for the pipeline failures and activity failures for both the pipelines. \\n \\n \\n \\nCREATE ACTION GROUP: \\n✓ Created action group for notification alerts through email.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='Orders Batch processing Notebook:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='OUTPUT TABLE ( FACT ORDER ): \\n \\n \\nPayments Batch processing Notebook:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='OUTPUT TABLE ( PAYMENTS ):\\n  \\n \\nSTREAM– PROCESSING: \\n \\n✓ Create an Event hub to collect events and send to the databricks notebook'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Create a new event in the event hub namespace “events” \\n \\n✓ Create a notebook and make connection with storage account.  \\n✓ USE THE AZURE KEY – VAULT to STORE THE SECRETS. \\n \\n✓ Connect to the event hub'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Schema define for the data \\n \\n \\n✓ Loading data to the bronze layer which are received from the event hub events. \\n✓ Separate the events as per the orders and Inventory data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Broze to silver after cleaning and validating the data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Gold to snowflake table \\n \\n \\n \\n \\n✓ INVENTORY UPDATE TO THE SNOWFLAKE INVENTORY TABLE'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='SNOWFLAKE WAREHOUSE DATA TABLE: \\n✓ E - R DIAGRAM: \\n \\n✓ Snowflake tables'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 15, 'page_label': '16', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Loading data from ADLS to a stage in Snowflake and copy into the tables \\n \\nCUSTOMER DATA SECURITY: \\nPII MASKING: \\n✓ For the customer table mask the email and phone number \\n \\n \\nCustomer Table:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='AZURE KEY VAULT: \\n✓ Created a Azure key vault \\n \\n✓ Created secrets to use it in the databricks by using Scope \\n \\n✓ Created  a scope inside the Databricks storage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='MONITORING & ERROR HANDLING: \\nMONITORING : \\n✓ Create a log analytics workspace \\n \\n✓ Open LOGS and then use KQL mode \\n \\n✓ Query the logs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='ALERTS: \\n✓ Create alerts for pipeline failures and activity failures \\n \\n✓ I have received an alert when the pipeline was failed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='POWER BI: \\n✓ Received more payments through CARD \\n \\n \\n✓ TOP 10 HIGHEST PAYMENT ORDERS:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 20, 'page_label': '21', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='GIT CONFIGURATION: \\n✓ Connected to my personal git account \\n \\n✓ Published it into my repository'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Git Repository Updation \\n \\n \\nCOST ESTIMATION: \\nAZURE COST: \\n \\n \\n✓ The DataBricks Has Comsumed 2015.98 in INR.  \\n✓ DataBricks has consumed more than 60% of the total cost.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='SNOWFLAKE COST: \\n✓ Compute cost occurred and costed about 8.9 credits \\n \\n✓ Total cost occurred is about 32 dollars( 2700 INR )'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='COMPARISION BETWEEN AZURE AND SNOWFLAKE: \\n✓ For this project the Azure costed 3200 INR and SNOWFLAKE costed 2700 INR \\n✓ I have used more AZURE SERVICES when compared to SNOWFLAKE \\n✓ SNOWFLAKE is Costlier than the AZURE. \\n \\nSTEPS OR MEASURES FOR COST OPTIMIZATION: \\nAzure \\n• Storage: Use Hot/Cool/Archive tiers + lifecycle policies. \\n• ADF: Use auto-pause on Integration Runtime, avoid too many activities. \\n• Databricks: Enable auto-termination, use job clusters, pick smaller/spot nodes. \\n• Event Hub: Scale down throughput units, set short retention, archive to Blob. \\n \\nSnowflake \\n• Warehouses: Enable auto-suspend/auto-resume, size warehouses correctly. \\n• Storage: Limit Time Travel, drop unused/stale data, load compressed files. \\n• Queries: Avoid SELECT *, use result caching & clustering keys. \\n• Monitoring: Set up Resource Monitors to cap credit usage.')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x12df5f230>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x133aedfd0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store1\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='RETAIL – CASE - STUDY \\nNAME: Govind Polireddi      Employee Id: AS1552 \\n \\nProblem Statement \\nRetail companies struggle with integrating in-store POS transactions, online orders, and \\ninventory systems into a single analytics-ready platform. \\nThey need: \\n• Batch processing for daily sales reconciliation. \\n• Real-time ingestion for fraud detection & stock alerts. \\n• Cloud-native scalability to support peak season sales. \\n• Unified warehouse (Snowflake / Synapse) for advanced reporting  \\n• Monitoring & error handling for operational resilience. \\n• Data security for customer PII. \\n• Cost transparency for infrastructure & query optimization. \\n \\nAPPROACH: \\n➢ Step-1: Creating a ADLS storage account to store the raw data and also the cleaned \\n and validated data in medallion structure ( Bronze, Silver, Gold ). \\n➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for \\n              each of the Batch Orders CSV and Payments CSV files.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for \\n              each of the Batch Orders CSV and Payments CSV files.  \\n➢ Step-3: Use Event- Grid to create trigger the pipelines when ever there is new file     \\n               uploaded. \\n➢ Step-4: Copy the data to the bronze by using COPY activity, after clean and validate  \\nthe data and store it in the Silver and Gold layer and finally load it into the  \\nSnowflake table. \\n➢ Step-4: Create Alerts from ADF monitor and LOG the DATABRICKS logs into LOG  \\n              ANALYTICS WORKSPACE. \\n➢ Step-5: For STREAM-PROCESSING create a Event-hub to collect the events from the \\n              Kafka. Create Databricks Notebook and connect to the Event-hub. Collect the \\n events from event hub and  process the event and store the data in delta  \\nformat in bronze, silver and gold. Finally connect the snowflake toload the  \\ndata into the fact table.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='events from event hub and  process the event and store the data in delta  \\nformat in bronze, silver and gold. Finally connect the snowflake toload the  \\ndata into the fact table. \\n➢ Step-6: Load the customer, product and store data from ADLS to the snowflake by'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='ADLS SAS token and creating stage in the snowflake to copy the data. Use \\ncopy into the Snowflake table. \\n➢ Step-7: For the customer data do the PII masking for the Email and phone number   \\nfor security purpose. \\n➢ Step-8: Connect snowflake with Power BI for the real time analytics. \\n \\nARCHITECTURE: \\nI am using the below architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='AZURE STORAGE ACCOUNT CREATION: \\n✓ Created an storage account “azurestorageaccount52”.  \\n \\n✓ Create a container and folders in it to store the data in medallion Architecture. \\n \\n✓ Store the customer, products and store data in raw directory \\n \\n✓ The raworders and rawpayment folders are for the storage of Batch process files \\nautomatically when there is a new file uploaded.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='BATCH – PROCESSING: \\nFor Batch orders pipeline. \\n✓ Create a pipeline for batch orders to run when a new file is added in raworders. \\n \\nFor Batch payment pipeline \\n✓ Create a pipeline for batch payment to run when a new file is added in rawpayments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Create 2 triggers to run the pipelines for new file upload. \\n \\n \\n✓ Create Alerts for the pipeline failures and activity failures for both the pipelines.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='CREATE ACTION GROUP: \\n✓ Created action group for notification alerts through email. \\n \\n \\n \\nOrders Batch processing Notebook:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='OUTPUT TABLE ( FACT ORDER ):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='Payments Batch processing Notebook: \\n \\n \\n \\n \\nOUTPUT TABLE ( PAYMENTS ):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='STREAM– PROCESSING: \\n✓ Create an Event hub to collect events and send to the databricks notebook \\n \\n✓ Create a new event in the event hub namespace “events” \\n \\n✓ Create a notebook and make connection with storage account.  \\n✓ USE THE AZURE KEY – VAULT to STORE THE SECRETS.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Connect to the event hub  \\n \\n✓ Schema define for the data \\n \\n \\n✓ Loading data to the bronze layer which are received from the event hub events. \\n✓ Separate the events as per the orders and Inventory data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Broze to silver after cleaning and validating the data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Gold to snowflake table'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ INVENTORY UPDATE TO THE SNOWFLAKE INVENTORY TABLE \\n \\n \\nSNOWFLAKE WAREHOUSE DATA TABLE: \\n✓ E - R DIAGRAM:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Snowflake tables'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Loading data from ADLS to a stage in Snowflake and copy into the tables \\n \\nPII MASKING: \\n✓ For the customer table mask the email and phone number'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='Customer Table: \\n \\n \\nMONITORING : \\n✓ Create a log analytics workspace \\n \\n✓ Open LOGS and then use KQL mode'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='✓ Query the logs  \\n \\n \\nALERTS: \\n✓ Create alerts for pipeline failures and activity failures'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T13:41:50+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T13:41:50+05:30', 'source': '../data/pdffiles/RETAIL_project.pdf', 'total_pages': 18, 'page': 17, 'page_label': '18', 'source_file': 'RETAIL_project.pdf', 'file_type': 'pdf'}, page_content='POWER BI: \\n✓ Received more payments through CARD'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='AWS ASSESSMENT\\nPOLIREDDI GOVIND                                                                                 AS1552\\nProblem Statement: \\nBuild a Batch Data Pipeline to Ingest CSV Data from S3 → Process with Glue → Load to Redshift → Query via Athena\\nSTEP 1:  SERVICE CREATION + IAM ROLES\\n➢ Create  three  S3 buckets (Source CSEV storage, Failed CSV storage, Processed CSV storage) to store the CSV ﬁles.\\n➢ Create a GlueRole in IAM Roles with AWSGlueServiceRole and AmazonS3FullAccess policies.\\n➢ Create a RedshiftRole in IAM Roles with AmazonS3ReadOnlyAccess and AWSGlueConsoleFullAccess policies.\\n➢ Create a new Redshift serverless namespace and workgroup, attach the RedshiftRole to grant the necessary permissions \\nto access the S3 and AWS Glue Data Catalog.\\n➢ Create a Glue Crawler and add the S3 bucket as data source, attach GlueRole policy to grant the required permissions to \\naccess the data.\\nSTEP 2:  AUTOMATION WITH LAMBDA + STEP FUNCTIONS'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='➢ Create a Glue Crawler and add the S3 bucket as data source, attach GlueRole policy to grant the required permissions to \\naccess the data.\\nSTEP 2:  AUTOMATION WITH LAMBDA + STEP FUNCTIONS\\n➢ When ever a new CSV ﬁle is added to the S3 bucket a LAMBDA function will get triggered.\\nCSV S3  \\nBUCKET\\nGLUE  \\nETL \\nJOB\\nRED SHIFT ATHENA'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='➢ The LAMBDA function starts the AWS Step Functions  workﬂow\\n➢ Step Functions orchestrate the pipeline:\\n• Start Glue Job and Monitor job completion.\\n• Trigger next steps automatically.\\n➢ Failed ﬁles will be directed to Failed_CSV_S3 storage and then by using AWS SNS a notiﬁcation will be sent.\\nSTEP 3: CRAWLER + DATA PROCESSING\\n➢ The raw CSV data stored in the Amazon S3 bucket will be crawled then metadata and schema will be stored in GLUE \\nCATALOG and then ingested to an AWS Glue ETL job.\\n➢ The Glue job will extract, clean, and transform the data, then convert it into an Parquet format. \\n➢ The processed Parquet ﬁles will be stored in a separate Amazon S3 bucket.\\nSTEP 4: LOADING PARQUT FILES INTO AMAZON REDSHIFT\\n➢ The processed Parquet ﬁles from the new Amazon S3 bucket are loaded into Amazon Redshift with(COPY/Auto COPY \\nfrom processed Parquet).\\n➢ Use IAM role to allow Redshift to read processed bucket.\\nSTEP 5: CONNECTING TO AMAZON ATHENA'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='from processed Parquet).\\n➢ Use IAM role to allow Redshift to read processed bucket.\\nSTEP 5: CONNECTING TO AMAZON ATHENA\\n➢ Deﬁne Athena Workgroup and connect it with Redshift Spectrum for querying processed data.\\nCloudFormation for the Pipeline: \\nThis CloudFormation stack sets up an end-to-end data processing pipeline on AWS. It provisions the required S3 \\nbuckets, IAM roles, Glue crawlers, Glue ETL jobs, and Redshift Serverless resources. The pipeline automates \\ningestion of CSV ﬁles using Lambda and Step Functions, transforms them into Parquet format, and loads the \\nprocessed data into Amazon Redshift and integrates with Amazon Athena for querying and analytics.'),\n",
       " Document(metadata={'producer': 'macOS Version 15.6.1 (Build 24G90) Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20250829064509Z00'00'\", 'title': 'aws', 'moddate': \"D:20250829064509Z00'00'\", 'source': '../data/pdffiles/aws.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'aws.pdf', 'file_type': 'pdf'}, page_content='ARCHITECTURE'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='SNOWFLAKE ASSIGNMENT \\n \\nName: Polireddi Govind Employee Id: AS1552 \\n \\nProblem Statement 1: \\nAutomate Real-time Log Data Ingestion from AWS S3 into \\nSnowflake using Snowpipe and Monitor via Snowsight  \\n \\nTopics Covered: Snowpipe, Internal/External Stage, Copy Options, \\nStreaming, SnowSQL, Performance Optimization, Virtual Warehouse, \\nCaching, Monitoring (Snowsight), AWS S3 Integration, Time Travel,  \\nClustering, Data Sampling \\n \\nProject Overview: \\nThis project aims to automate real-time log data ingestion from AWS S3 into Snowflake using Snowpipe, \\nestablishing a seamless, serverless, and near-instant data loading pipeline. By connecting Snowflake with S3 \\nevent notifications, newly uploaded log files are automatically ingested, processed, and made available for \\nanalytics with minimal manual effort. The solution focuses on performance optimization, cost efficiency, and \\noperational visibility, leveraging Snowflake’s capabilities such as external/internal stages, copy options, virtual'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='operational visibility, leveraging Snowflake’s capabilities such as external/internal stages, copy options, virtual \\nwarehouses, clustering, caching, time travel, and Snowsight monitoring.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='Architecture Overview \\n1. Log Producers → AWS S3 \\n• Application logs (e.g., auth, payments, orders, shipping) are written to S3 in JSON format, one log per \\nline. \\n• Each JSON contains fields like: \\no log_id, timestamp, level, service, message, user_id, ip_address.  \\n• File size target: 10–200 MB compressed (parquet) for efficient Snowpipe ingestion. \\n \\n2. S3 Event Notifications → Snowflake Snowpipe \\n• Each new file triggers an S3 event notification (via SNS/SQS). \\n• Snowpipe automatically loads data into Snowflake without manual intervention. \\n3. Snowflake Staging and Curation \\n• External Stage: Secure pointer to the S3 bucket/prefix. \\n• Snowpipe: Automatically ingests JSON logs into a raw staging table. \\n• Curation Layer: Transforms, normalizes, and deduplicates data into curated analytics tables. \\n4. Monitoring and Optimization \\n• Snowsight: Monitors ingestion latency, file load history, and error counts. \\n• Virtual Warehouses: Handle downstream transformations.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='4. Monitoring and Optimization \\n• Snowsight: Monitors ingestion latency, file load history, and error counts. \\n• Virtual Warehouses: Handle downstream transformations. \\n• Clustering & Time Travel: Improve query speed and allow recovery from data issues.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content=\"Step-by-Step Approach for Implementation  \\nStep 1: Data Landing on S3 \\n• Logs generated by different services are streamed to the S3 bucket.  \\n• Files are batched and upload to the S3 bucket. \\n                              \\nStep 2: Setting Up the Snowflake Stage \\n• Create an AWS IAM role with GetObject and ListBucket permissions. \\n• Stage points securely to the S3 log location. \\n                   \\nStep 3: Defining File Format and Copy Options \\n• Define a JSON file format to handle the log data structure: \\n• COPY options: \\no Use ON_ERROR='CONTINUE' to skip corrupted records. \\no Enable PURGE=FALSE for reprocessing flexibility. \\nStep 4: Automating Ingestion with Snowpipe\"),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='• Create a Snowpipe with AUTO_INGEST = TRUE: \\n• Configure S3 event notifications to publish to the Snowflake pipe endpoint. \\n• Each new JSON file is ingested into the raw_logs table. \\n                               \\nStep 5: Transforming and Curating Data \\n• Parse JSON data into structured columns: \\n• Apply deduplication using DISTINCT or a merge on log_id. \\n• Add clustering keys: (event_time, service_name).  \\n• Use Time Travel for data correction and auditing. \\n                                \\nStep 6: Performance Optimization \\n• Avoid small files — batch logs into optimal file sizes. \\n• Use clustering and search optimization for fast filtering (e.g., by service_name or log_level).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='• Enable caching for repeated dashboard queries.  \\n• Periodically re-cluster curated tables to maintain performance. \\n                                     \\nStep 7: Monitoring with Snowsight \\n• Use the Pipes Dashboard to track: \\no Ingestion latency \\no Success/failure rates \\no File queue backlog \\n• Set up custom KPIs: \\no Ingestion delay = CURRENT_TIMESTAMP - file_last_modified \\no Failed file counts, ingestion throughput \\n• Configure email/SMS alerts for load errors or pipe downtime.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='Step 8: Backfills and Reprocessing \\n• Use COPY INTO for historical or missed data loads: \\n• Pause/resume AUTO_INGEST during maintenance.  \\n• Use Time Travel and Fail-safe for replay or compliance scenarios. \\n \\nStep 9: Advanced Capabilities \\n• Snowpipe Streaming: For sub-second ingestion directly from producers (optional). \\n• Data Sampling: Use SAMPLE or partition filters for debugging.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='• Cost Control: \\no Use Snowpipe for ingestion (Snowflake -managed compute). \\no Use auto-suspend warehouses for transformation to minimize compute cost.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='ARCHITECTURE: \\nS3 → SQS → Snowpipe  → Snowflake Stage → Copy Into Logs Table (Snowflake)  \\n \\n  \\nIMPLEMENTATION \\n \\nS3 BUCKET: \\n✓ Created a S3 bucket “snowflake-logs-data-bucket” \\n \\nSNOWFAKE WAREHOUSE: \\n✓ Created a Warehouse (Sigmoid) , Database(logsdb) and Schema (RAW). \\n \\n✓ Created a log_events Table to store the logs from S3. \\n✓ Create a JSON file format.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='S3 READ POLICY(Role): \\n✓ Create a new role in IAM for S3 read access and assign to the Snowflake. \\n \\n \\n \\nS3 and SNOWFLAKE INTEGRATION: \\n✓ Intregrate S3 and Snowflake, assign the role and the location of the bucket. \\n \\n✓ Describe the created integration. \\n \\n✓ Copy the ARN and External_ID from the OUTPUT.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Update the Policy with ARN and External_ID in the Role policy. \\n \\nSTAGE CREATION: \\n✓ Create a stage to point it to the S3 bucket to fetch the Json files. \\n \\n✓ Successfully created a stage. \\n \\nSNOWPIPE: \\n✓ Create a Snowpipe to ingest the data continuously into the logs table.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Lineage graph is created from the Stage to the Table \\n \\n \\n \\nSQS EVENT NOTIFICATION: \\n✓ Create SQS Event notification ,when there is a new file added into the S3 bucket it \\ngets trigger the Snowpipe. \\n \\n✓ Successfully created the SQS notification.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='FILE UPLOAD: \\n✓ Uploaded a “logs_dataset1.json” file to the S3 bucket. \\n \\n✓ Successfully loaded the logs data into the Table (100 Rows) \\n \\n \\n✓ Uploaded a “logs_dataset2.json” file to the S3 bucket.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Successfully loaded the logs data into the Table (200 Rows) \\n \\n \\nTIME TRAVEL: \\n✓ Retention period is 7 days. \\n \\n✓ Table logs data is of 200 Rows.(Present) \\n \\n✓ Table logs data is of 100 Rows.(past of [400] seconds )'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='CACHE AND QUERY OPTIMIZATION: \\n✓ Cluster the Column(timestamp) for faster optimization. \\n \\n✓ 90ms time taken to retrieve the data as per the Query. \\n \\n✓ 38ms time taken to retrieve the data as per the Query as it cached the Result(Result \\nCache). \\n \\n \\n \\nMONITORING(SNOWSIGHT):  \\n✓ Number of logs per day. 28th September got more logs.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251104164940', 'source': '../data/pdffiles/Govind_AS1552_Snowflake_Asgn.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'Govind_AS1552_Snowflake_Asgn.pdf', 'file_type': 'pdf'}, page_content='✓ Error is having the main issue in the logs. \\n \\n✓ Bar chart showing the message wise total logs.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT    1.   Why  I  pasted  it:  To  show  the  raw  and  processed  storage  locations  I  set  up  for      \\nmy\\n \\npipeline.\\n \\n What  I  did  there:  Created  an  S3  bucket  named  my-store-sales-pipeline with  \\nfolders\\n RAW/ for  input  JSON  files,  Processed/ for  Parquet  outputs,  and  athena_results/ for  query  results.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT    \\n      2.  Why  I  pasted  it:  To  demonstrate  that  I  configured  Athena  to  use  the  Glue  Data  \\nCatalog\\n \\ndatabase\\n \\nfor\\n \\nquerying\\n \\nprocessed\\n \\ndata.\\n \\n What  I  did  there:  Opened  Athena  Query  Editor  and  confirmed  that  the  database  store_sales_db is  visible  and  available  for  running  queries.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT    \\n   3.  Why  I  pasted  it:  To  show  the  local  environment  setup  needed  to  run  Airflow  \\nwithout\\n \\nusing\\n \\npaid\\n \\nMWAA.\\n \\n What  I  did  there:  Installed  Docker  Desktop,  pulled  the  official  Airflow  Docker  \\nCompose\\n \\nsetup,\\n \\nand\\n \\nprepared\\n \\nthe\\n dags/,  plugins/,  and  requirements/ folders  for  \\nthe\\n \\nproject.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT      4.  Why  I  pasted  it:  To  demonstrate  that  my  DAG  executed  successfully,  \\norchestrating\\n \\nthe\\n \\nJSON-to-Parquet\\n \\nconversion\\n \\nand\\n \\nAthena\\n \\ntable\\n \\nrefresh.\\n \\n What  I  did  there:  Triggered  the  store_sales_reporting DAG  in  the  Airflow  UI,  \\nand\\n \\nboth\\n \\ntasks\\n \\n(convert_json_to_parquet and  run_athena_query)  completed  \\nsuccessfully.\\n  \\n   5.  Why  I  pasted  it:  To  validate  that  the  DAG  successfully  transformed  raw  JSON  \\nfiles\\n \\ninto\\n \\nParquet\\n \\nformat\\n \\nand\\n \\nstored\\n \\nthem\\n \\nback\\n \\nin\\n \\nS3.\\n \\n What  I  did  there:  Checked  the  Processed/ folder  inside  my  S3  bucket  and  \\nconfirmed\\n \\nthat\\n \\nnew\\n .parquet files  were  created  after  the  DAG  run.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m142 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Airflow', 'source': '../data/pdffiles/Airflow_tejas_final.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7', 'source_file': 'Airflow_tejas_final.pdf', 'file_type': 'pdf'}, page_content='NAME:-  Tejas  Singh  Emp  ID:  AS1554   \\n                             \\nAIRFLOW  ASSIGNMENT'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='RETAIL – CASE - STUDY \\nNAME: Govind Polireddi      Employee Id: AS1552 \\n \\nProblem Statement \\nRetail companies struggle with integrating in-store POS transactions, online orders, and \\ninventory systems into a single analytics-ready platform. \\nThey need: \\n• Batch processing for daily sales reconciliation. \\n• Real-time ingestion for fraud detection & stock alerts. \\n• Cloud-native scalability to support peak season sales. \\n• Unified warehouse (Snowflake / Synapse) for advanced reporting  \\n• Monitoring & error handling for operational resilience. \\n• Data security for customer PII. \\n• Cost transparency for infrastructure & query optimization. \\n \\nI am Using LAMBDA ARCHITECTURE as it supports both BATCH AND STREAMING PROCESS. \\n \\nAPPROACH: \\n➢ Step-1: Creating a ADLS storage account to store the raw data and also the cleaned \\n and validated data in medallion structure ( Bronze, Silver, Gold ). \\n➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='and validated data in medallion structure ( Bronze, Silver, Gold ). \\n➢ Step-2: For BATCH-PROCESSING create 2 ADF(Azure Data Factory) Pipelines one for \\n              each of the Batch Orders CSV and Payments CSV files.  \\n➢ Step-3: Use Event- Grid to create trigger the pipelines when ever there is new file     \\n               uploaded. \\n➢ Step-4: Copy the data to the bronze by using COPY activity, after clean and validate  \\nthe data and store it in the Silver and Gold layer and finally load it into the  \\nSnowflake table. \\n➢ Step-4: Create Alerts from ADF monitor and LOG the DATABRICKS logs into LOG  \\n              ANALYTICS WORKSPACE. \\n➢ Step-5: For STREAM-PROCESSING create a Event-hub to collect the events from the \\n              Kafka. Create Databricks Notebook and connect to the Event-hub. Collect the \\n events from event hub and  process the event and store the data in delta'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='format in bronze, silver and gold. Finally connect the snowflake toload the  \\ndata into the fact table. \\n➢ Step-6: Load the customer, product and store data from ADLS to the snowflake by \\nADLS SAS token and creating stage in the snowflake to copy the data. Use \\ncopy into the Snowflake table. \\n➢ Step-7: For the customer data do the PII masking for the Email and phone number   \\nfor security purpose. \\n➢ Step-8: Connect snowflake with Power BI for the real time analytics. \\n \\n \\nARCHITECTURE: \\nI am using the below architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='AZURE STORAGE ACCOUNT CREATION: \\n✓ Created an storage account “azurestorageaccount52”.  \\n \\n✓ Create a container and folders in it to store the data in medallion Architecture. \\n \\n✓ Store the customer, products and store data in raw directory \\n \\n✓ The raworders and rawpayment folders are for the storage of Batch process files \\nautomatically when there is a new file uploaded.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='BATCH – PROCESSING: \\nFor Batch orders pipeline. \\n \\n✓ Create a pipeline for batch orders to run when a new file is added in raworders. \\n \\n \\n \\nFor Batch payment pipeline'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Create a pipeline for batch payment to run when a new file is added in rawpayments. \\n \\n✓ Create 2 triggers to run the pipelines for new file upload.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Create Alerts for the pipeline failures and activity failures for both the pipelines. \\n \\n \\n \\nCREATE ACTION GROUP: \\n✓ Created action group for notification alerts through email.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='Orders Batch processing Notebook:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='OUTPUT TABLE ( FACT ORDER ): \\n \\n \\nPayments Batch processing Notebook:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='OUTPUT TABLE ( PAYMENTS ):\\n  \\n \\nSTREAM– PROCESSING: \\n \\n✓ Create an Event hub to collect events and send to the databricks notebook'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Create a new event in the event hub namespace “events” \\n \\n✓ Create a notebook and make connection with storage account.  \\n✓ USE THE AZURE KEY – VAULT to STORE THE SECRETS. \\n \\n✓ Connect to the event hub'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Schema define for the data \\n \\n \\n✓ Loading data to the bronze layer which are received from the event hub events. \\n✓ Separate the events as per the orders and Inventory data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Broze to silver after cleaning and validating the data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Gold to snowflake table \\n \\n \\n \\n \\n✓ INVENTORY UPDATE TO THE SNOWFLAKE INVENTORY TABLE'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='SNOWFLAKE WAREHOUSE DATA TABLE: \\n✓ E - R DIAGRAM: \\n \\n✓ Snowflake tables'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 15, 'page_label': '16', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Loading data from ADLS to a stage in Snowflake and copy into the tables \\n \\nCUSTOMER DATA SECURITY: \\nPII MASKING: \\n✓ For the customer table mask the email and phone number \\n \\n \\nCustomer Table:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='AZURE KEY VAULT: \\n✓ Created a Azure key vault \\n \\n✓ Created secrets to use it in the databricks by using Scope \\n \\n✓ Created  a scope inside the Databricks storage'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='MONITORING & ERROR HANDLING: \\nMONITORING : \\n✓ Create a log analytics workspace \\n \\n✓ Open LOGS and then use KQL mode \\n \\n✓ Query the logs'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='ALERTS: \\n✓ Create alerts for pipeline failures and activity failures \\n \\n✓ I have received an alert when the pipeline was failed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='POWER BI: \\n✓ Received more payments through CARD \\n \\n \\n✓ TOP 10 HIGHEST PAYMENT ORDERS:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 20, 'page_label': '21', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='GIT CONFIGURATION: \\n✓ Connected to my personal git account \\n \\n✓ Published it into my repository'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='✓ Git Repository Updation \\n \\n \\nCOST ESTIMATION: \\nAZURE COST: \\n \\n \\n✓ The DataBricks Has Comsumed 2015.98 in INR.  \\n✓ DataBricks has consumed more than 60% of the total cost.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='SNOWFLAKE COST: \\n✓ Compute cost occurred and costed about 8.9 credits \\n \\n✓ Total cost occurred is about 32 dollars( 2700 INR )'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-22T18:24:54+05:30', 'author': 'Polireddi Govind', 'moddate': '2025-09-22T18:24:54+05:30', 'source': '../data/pdffiles/Govind RETAIL Documentation.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24', 'source_file': 'Govind RETAIL Documentation.pdf', 'file_type': 'pdf'}, page_content='COMPARISION BETWEEN AZURE AND SNOWFLAKE: \\n✓ For this project the Azure costed 3200 INR and SNOWFLAKE costed 2700 INR \\n✓ I have used more AZURE SERVICES when compared to SNOWFLAKE \\n✓ SNOWFLAKE is Costlier than the AZURE. \\n \\nSTEPS OR MEASURES FOR COST OPTIMIZATION: \\nAzure \\n• Storage: Use Hot/Cool/Archive tiers + lifecycle policies. \\n• ADF: Use auto-pause on Integration Runtime, avoid too many activities. \\n• Databricks: Enable auto-termination, use job clusters, pick smaller/spot nodes. \\n• Event Hub: Scale down throughput units, set short retention, archive to Blob. \\n \\nSnowflake \\n• Warehouses: Enable auto-suspend/auto-resume, size warehouses correctly. \\n• Storage: Limit Time Travel, drop unused/stale data, load compressed files. \\n• Queries: Avoid SELECT *, use result caching & clustering keys. \\n• Monitoring: Set up Resource Monitors to cap credit usage.')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 73 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (73, 384)\n",
      "Adding 73 documents to vector store...\n",
      "Successfully added 73 documents to vector store\n",
      "Total documents in collection: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x133aee270>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is SNOWFLAKE COST'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_6746bac9_71',\n",
       "  'content': 'SNOWFLAKE COST: \\n✓ Compute cost occurred and costed about 8.9 credits \\n \\n✓ Total cost occurred is about 32 dollars( 2700 INR )',\n",
       "  'metadata': {'total_pages': 24,\n",
       "   'producer': 'Microsoft® Word 2021',\n",
       "   'moddate': '2025-09-22T18:24:54+05:30',\n",
       "   'page': 22,\n",
       "   'creator': 'Microsoft® Word 2021',\n",
       "   'page_label': '23',\n",
       "   'author': 'Polireddi Govind',\n",
       "   'content_length': 126,\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2025-09-22T18:24:54+05:30',\n",
       "   'source': '../data/pdffiles/Govind RETAIL Documentation.pdf',\n",
       "   'doc_index': 71,\n",
       "   'source_file': 'Govind RETAIL Documentation.pdf'},\n",
       "  'similarity_score': 0.4752863645553589,\n",
       "  'distance': 0.5247136354446411,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_c858a447_13',\n",
       "  'content': '✓ Gold to snowflake table',\n",
       "  'metadata': {'creator': 'Microsoft® Word 2021',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2025-09-22T13:41:50+05:30',\n",
       "   'page': 11,\n",
       "   'content_length': 25,\n",
       "   'doc_index': 13,\n",
       "   'author': 'Polireddi Govind',\n",
       "   'producer': 'Microsoft® Word 2021',\n",
       "   'source_file': 'RETAIL_project.pdf',\n",
       "   'page_label': '12',\n",
       "   'moddate': '2025-09-22T13:41:50+05:30',\n",
       "   'total_pages': 18,\n",
       "   'source': '../data/pdffiles/RETAIL_project.pdf'},\n",
       "  'similarity_score': 0.01027822494506836,\n",
       "  'distance': 0.9897217750549316,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is SNOWFLAKE COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gemma2-9b-it\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4110c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_bd5cc745_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '3',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'content_length': 941,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_442dbe7b_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 61,\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'content_length': 941,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_45e2cfa9_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 941,\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'file_type': 'pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'page_label': '3',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3497d198_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'page': 4,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 937,\n",
       "   'doc_index': 71,\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bb00aef7_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'content_length': 937,\n",
       "   'page': 4,\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'doc_index': 71,\n",
       "   'page_label': '5',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea465ac",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df1bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attention mechanism is a function that maps a query and a set of key-value pairs to an output vector, using a weighted sum of the values.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b1c2",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2832fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Hard Negative Mining Technqiues'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 95.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The text describes several hard negative mining techniques used in contrastive learning for retrieval models:\n",
      "\n",
      "* **ANCE:** Uses asynchronous ANN indexing and checkpoint states to periodically update hard negatives.\n",
      "* **Conan-Embedding:** Employs a dynamic strategy, excluding and refreshing samples based on score thresholds.\n",
      "* **NV-Retriever:**  Proposes positive-aware mining with TopK-MarginPos and TopKPercPos filtering to reduce false negatives.\n",
      "* **LGAI-Embedding:** Builds on NV-Retriever, using ANNA IR as a teacher model to identify high-quality hard negatives and TopKPercPos filtering. \n",
      "\n",
      "\n",
      "\n",
      "Sources: [{'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}]\n",
      "Confidence: 0.18709993362426758\n",
      "Context Preview: QZhou-Embedding Technical Report\n",
      " Kingsoft AI\n",
      "2.4 Hard Negative Mining Techniques\n",
      "Hard negatives serve as essential components in contrastive lear ning for retrieval model\n",
      "training. Early work like ANCE[\n",
      "46] proposed an asynchronous ANN indexing mech-\n",
      "anism that periodically updates hard negatives u\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa6150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention is all you need'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention functi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "Question: what is attention is all you need\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: \"Attention Is All You Need\" is a paper that introduced the Transformer model, which relies solely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional networks.  \n",
      "\n",
      "\n",
      "Citations:\n",
      "[1] attention.pdf (page 2)\n",
      "[2] attention.pdf (page 2)\n",
      "[3] attention.pdf (page 2)\n",
      "Summary: The paper \"Attention Is All You Need\" introduced the Transformer model, a novel architecture for sequence transduction tasks.  This model utilizes only attention mechanisms, dispensing with traditional recurrent or convolutional networks. \n",
      "\n",
      "\n",
      "\n",
      "History: {'question': 'what is attention is all you need', 'answer': '\"Attention Is All You Need\" is a paper that introduced the Transformer model, which relies solely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional networks.  \\n', 'sources': [{'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}], 'summary': 'The paper \"Attention Is All You Need\" introduced the Transformer model, a novel architecture for sequence transduction tasks.  This model utilizes only attention mechanisms, dispensing with traditional recurrent or convolutional networks. \\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
